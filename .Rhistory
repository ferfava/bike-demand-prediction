library(tidyverse)
library(lubridate)
library(keras)
library(recipes)
library(tensorflow)
library(rsample)
# 🛠 1. Instalar paquetes (ejecutar una sola vez)
packages <- c("tidyverse", "lubridate", "keras", "tensorflow",
"recipes", "rsample", "yardstick", "vip")
install_if_missing <- function(p) {
if (!requireNamespace(p, quietly = TRUE)) {
install.packages(p)
}
}
invisible(lapply(packages, install_if_missing))
library(tidyverse)
library(lubridate)
library(keras)
library(tensorflow)
library(recipes)
library(rsample)
library(yardstick)
library(vip)
data <- read_csv("hour.csv")
glimpse(data)
summary(data$cnt)
data_clean <- data %>%
mutate(
hr_sin = sin(2 * pi * hr / 24),
hr_cos = cos(2 * pi * hr / 24),
weekday = as.factor(weekday),
season = as.factor(season),
weathersit = as.factor(weathersit),
workingday = as.factor(workingday)
) %>%
select(cnt, temp, atemp, hum, windspeed, hr_sin, hr_cos,
season, weathersit, workingday)
set.seed(123)
split <- initial_split(data_clean, prop = 0.8)
train_data <- training(split)
test_data  <- testing(split)
# 🍳 7. Preparar receta (normalización + dummies)
rec <- recipe(cnt ~ ., data = train_data) %>%
step_dummy(all_nominal()) %>%
step_normalize(all_predictors()) %>%
prep()
x_train <- bake(rec, new_data = train_data) %>% select(-cnt) %>% as.matrix()
y_train <- train_data$cnt
x_test <- bake(rec, new_data = test_data) %>% select(-cnt) %>% as.matrix()
y_test <- test_data$cnt
model <- keras_model_sequential() %>%
layer_dense(units = 128, activation = "relu", input_shape = ncol(x_train)) %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 64, activation = "relu") %>%
layer_dense(units = 1)
library(keras)
install_tensorflow()
# 🧠 8. Crear la red neuronal
model <- keras_model_sequential() %>%
layer_dense(units = 128, activation = "relu", input_shape = ncol(x_train)) %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 64, activation = "relu") %>%
layer_dense(units = 1)
library(keras)
install_tensorflow()
install.packages("keras")  # Por si aún no lo tenés instalado
library(keras)
install_tensorflow()
library(keras)
# Verificamos que todo esté ok
is_keras_available()
# 🛠 1. Instalar paquetes (ejecutar una sola vez)
packages <- c("tidyverse", "lubridate", "keras", "tensorflow",
"recipes", "rsample", "yardstick", "vip")
install_if_missing <- function(p) {
if (!requireNamespace(p, quietly = TRUE)) {
install.packages(p)
}
}
invisible(lapply(packages, install_if_missing))
# 📦 2. Cargar paquetes
library(tidyverse)
library(lubridate)
library(keras)
library(tensorflow)
library(recipes)
library(rsample)
library(yardstick)
library(vip)
# 📁 3. Cargar los datos
data <- read_csv("hour.csv")
# 👁 4. Exploración rápida
glimpse(data)
summary(data$cnt)
# 🧼 5. Preprocesamiento y feature engineering
data_clean <- data %>%
mutate(
hr_sin = sin(2 * pi * hr / 24),
hr_cos = cos(2 * pi * hr / 24),
weekday = as.factor(weekday),
season = as.factor(season),
weathersit = as.factor(weathersit),
workingday = as.factor(workingday)
) %>%
select(cnt, temp, atemp, hum, windspeed, hr_sin, hr_cos,
season, weathersit, workingday)
# 🧪 6. Dividir en entrenamiento y prueba
set.seed(123)
split <- initial_split(data_clean, prop = 0.8)
train_data <- training(split)
test_data  <- testing(split)
# 🍳 7. Preparar receta (normalización + dummies)
rec <- recipe(cnt ~ ., data = train_data) %>%
step_dummy(all_nominal()) %>%
step_normalize(all_predictors()) %>%
prep()
x_train <- bake(rec, new_data = train_data) %>% select(-cnt) %>% as.matrix()
y_train <- train_data$cnt
x_test <- bake(rec, new_data = test_data) %>% select(-cnt) %>% as.matrix()
y_test <- test_data$cnt
library(keras)
install_tensorflow()
library(reticulate)
use_python("C:/Users/Noxi-PC/AppData/Local/Programs/Python/Python311/python.exe", required = TRUE)
library(keras)
# Crear un modelo secuencial simple
model <- keras_model_sequential() %>%
layer_dense(units = 16, activation = 'relu', input_shape = c(10)) %>%
layer_dense(units = 1)
# Mostrar resumen del modelo
summary(model)
# 🛠 1. Instalar paquetes (ejecutar una sola vez)
packages <- c("tidyverse", "lubridate", "keras", "tensorflow",
"recipes", "rsample", "yardstick", "vip")
install_if_missing <- function(p) {
if (!requireNamespace(p, quietly = TRUE)) {
install.packages(p)
}
}
invisible(lapply(packages, install_if_missing))
# 📦 2. Cargar paquetes
library(tidyverse)
library(lubridate)
library(keras)
library(tensorflow)
library(recipes)
library(rsample)
library(yardstick)
library(vip)
# 📁 3. Cargar los datos
data <- read_csv("hour.csv")
# 👁 4. Exploración rápida
glimpse(data)
summary(data$cnt)
# 🧼 5. Preprocesamiento y feature engineering
data_clean <- data %>%
mutate(
hr_sin = sin(2 * pi * hr / 24),
hr_cos = cos(2 * pi * hr / 24),
weekday = as.factor(weekday),
season = as.factor(season),
weathersit = as.factor(weathersit),
workingday = as.factor(workingday)
) %>%
select(cnt, temp, atemp, hum, windspeed, hr_sin, hr_cos,
season, weathersit, workingday)
# 🧪 6. Dividir en entrenamiento y prueba
set.seed(123)
split <- initial_split(data_clean, prop = 0.8)
train_data <- training(split)
test_data  <- testing(split)
# 🍳 7. Preparar receta (normalización + dummies)
rec <- recipe(cnt ~ ., data = train_data) %>%
step_dummy(all_nominal()) %>%
step_normalize(all_predictors()) %>%
prep()
x_train <- bake(rec, new_data = train_data) %>% select(-cnt) %>% as.matrix()
y_train <- train_data$cnt
x_test <- bake(rec, new_data = test_data) %>% select(-cnt) %>% as.matrix()
y_test <- test_data$cnt
# 🧠 8. Crear la red neuronal
model <- keras_model_sequential() %>%
layer_dense(units = 128, activation = "relu", input_shape = ncol(x_train)) %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 64, activation = "relu") %>%
layer_dense(units = 1)
library(reticulate)
use_python("C:/Users/Noxi-PC/AppData/Local/Programs/Python/Python311/python.exe", required = TRUE)
library(tensorflow)
install_tensorflow()
library(tensorflow)
install_tensorflow()
library(reticulate)
use_python("C:/Users/Noxi-PC/AppData/Local/Programs/Python/Python311/python.exe", required = TRUE)
library(keras)
library(tensorflow)
# 🛠 1. Instalar paquetes (ejecutar una sola vez)
packages <- c("tidyverse", "lubridate", "keras", "tensorflow",
"recipes", "rsample", "yardstick", "vip")
install_if_missing <- function(p) {
if (!requireNamespace(p, quietly = TRUE)) {
install.packages(p)
}
}
invisible(lapply(packages, install_if_missing))
# 📦 2. Cargar paquetes
library(tidyverse)
library(lubridate)
library(keras)
library(tensorflow)
library(recipes)
library(rsample)
library(yardstick)
library(vip)
# 📁 3. Cargar los datos
data <- read_csv("hour.csv")
# 👁 4. Exploración rápida
glimpse(data)
summary(data$cnt)
# 🧼 5. Preprocesamiento y feature engineering
data_clean <- data %>%
mutate(
hr_sin = sin(2 * pi * hr / 24),
hr_cos = cos(2 * pi * hr / 24),
weekday = as.factor(weekday),
season = as.factor(season),
weathersit = as.factor(weathersit),
workingday = as.factor(workingday)
) %>%
select(cnt, temp, atemp, hum, windspeed, hr_sin, hr_cos,
season, weathersit, workingday)
# 🧪 6. Dividir en entrenamiento y prueba
set.seed(123)
split <- initial_split(data_clean, prop = 0.8)
train_data <- training(split)
test_data  <- testing(split)
# 🍳 7. Preparar receta (normalización + dummies)
rec <- recipe(cnt ~ ., data = train_data) %>%
step_dummy(all_nominal()) %>%
step_normalize(all_predictors()) %>%
prep()
x_train <- bake(rec, new_data = train_data) %>% select(-cnt) %>% as.matrix()
y_train <- train_data$cnt
x_test <- bake(rec, new_data = test_data) %>% select(-cnt) %>% as.matrix()
y_test <- test_data$cnt
# 🧠 8. Crear la red neuronal
model <- keras_model_sequential() %>%
layer_dense(units = 128, activation = "relu", input_shape = ncol(x_train)) %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 64, activation = "relu") %>%
layer_dense(units = 1)
model %>% compile(
loss = "mse",
optimizer = optimizer_adam(learning_rate = 0.001),
metrics = c("mae")
)
# 📈 9. Entrenar el modelo
history <- model %>% fit(
x = x_train,
y = y_train,
epochs = 100,
batch_size = 64,
validation_split = 0.2,
callbacks = callback_early_stopping(patience = 10, restore_best_weights = TRUE)
)
# 📉 10. Evaluar el modelo
model %>% evaluate(x_test, y_test)
preds <- model %>% predict(x_test) %>% as.vector()
# 📊 11. Métricas
tibble(obs = y_test, pred = preds) %>%
metrics(truth = obs, estimate = pred)
# 🎨 12. Visualizar predicciones vs reales
ggplot(data.frame(y_test, preds), aes(x = y_test, y = preds)) +
geom_point(alpha = 0.4, color = "#00BFC4") +
geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
labs(title = "Predicción vs Real", x = "Valor real", y = "Valor predicho") +
theme_minimal()
# 🔍 13. Feature Importance (VIP)
vip(model, num_features = 10, method = "permute", train = x_test, target = y_test)
# 🔍 13. Feature Importance (VIP)
# Convertir x_test a data.frame con nombres de columnas
x_test_df <- bake(rec, new_data = test_data) %>% select(-cnt)
# Usar vip con métrica explícita (ej: "mae")
vip(
object = model,
num_features = 10,
method = "permute",
train = x_test_df,
target = y_test,
metric = "mae"
)
# 🔍 13. Feature Importance (VIP)
# Convertir x_test a data.frame con nombres de columnas
x_test_df <- bake(rec, new_data = test_data) %>% select(-cnt)
# Usar vip con métrica explícita y especificar la función de predicción
vip(
object = model,
num_features = 10,
method = "permute",
train = x_test_df,
target = y_test,
metric = "mae",
pred_wrapper = function(model, X) predict(model, X)  # Función para hacer predicciones
)
# 🔍 13. Feature Importance (VIP)
# Convertir x_test a data.frame con nombres de columnas
x_test_df <- bake(rec, new_data = test_data) %>% select(-cnt)
# Usar vip con métrica explícita y función de predicción con 'newdata'
vip(
object = model,
num_features = 10,
method = "permute",
train = x_test_df,
target = y_test,
metric = "mae",
pred_wrapper = function(object, newdata) {
predict(object, as.matrix(newdata)) %>% as.vector()
}
)
library(ggplot2)
vip_df %>%
slice_max(Importance, n = 10) %>%
ggplot(aes(x = reorder(Variable, Importance), y = Importance)) +
geom_col(fill = "#00BFC4") +
coord_flip() +
labs(
title = "Importancia de Variables (permute + MAE)",
x = "Variable",
y = "Importancia estimada"
) +
theme_minimal(base_size = 13)
vip_df <- vi(
object = model,
method = "permute",
train = x_test_df,
target = y_test,
metric = "mae",
pred_wrapper = function(object, newdata) {
predict(object, as.matrix(newdata)) %>% as.vector()
}
)
# Ver tabla
print(vip_df)
# Ver tabla
print(vip_df)
vip_df %>%
slice_max(Importance, n = 10) %>%
ggplot(aes(x = reorder(Variable, Importance), y = Importance)) +
geom_col(fill = "#00BFC4") +
coord_flip() +
labs(
title = "Importancia de Variables (permute + MAE)",
x = "Variable",
y = "Importancia estimada"
) +
theme_minimal(base_size = 13)
